{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iGMZxCkXXdKn",
    "outputId": "3fff6ce8-d92b-48e2-9520-19b113ee0596"
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/nilt43hyl1dx82k/dataset.zip?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4V-kwycpXtDJ",
    "outputId": "cadac7c5-947d-4a50-b2d6-eed25ffd6217"
   },
   "outputs": [],
   "source": [
    "!unzip dataset.zip?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dcwbXc8Xzww"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator , img_to_array, load_img\n",
    "from keras.applications.mobilenet import MobileNet, preprocess_input \n",
    "from keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQRxQ8qNbvE0"
   },
   "source": [
    "#  Building our Model To train the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7HCUQigdYELf",
    "outputId": "626e366c-2800-4311-aaa4-77a63ff6b57c"
   },
   "outputs": [],
   "source": [
    "# Working with pre trained model \n",
    "\n",
    "base_model = MobileNet( input_shape=(224,224,3), include_top= False )\n",
    "\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(units=7 , activation='softmax' )(x)\n",
    "\n",
    "# creating our model.\n",
    "model = Model(base_model.input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FN3kEpAeZUGj"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss= categorical_crossentropy , metrics=['accuracy']  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6kh9_hjbs47"
   },
   "source": [
    "# Preparing our data using data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpxBuUAlbmh8",
    "outputId": "82402687-0570-428d-af78-bdaad50444d9"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "     zoom_range = 0.2, \n",
    "     shear_range = 0.2, \n",
    "     horizontal_flip=True, \n",
    "     rescale = 1./255\n",
    ")\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(directory= \"/content/train\", \n",
    "                                               target_size=(224,224), \n",
    "                                               batch_size=32,\n",
    "                                  )\n",
    "\n",
    "\n",
    "train_data.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0x-TwUhocaHn",
    "outputId": "70922d05-e1c1-4eea-9fa9-631bca5dab53"
   },
   "outputs": [],
   "source": [
    "val_datagen = ImageDataGenerator(rescale = 1./255 )\n",
    "\n",
    "val_data = val_datagen.flow_from_directory(directory= \"/content/test\", \n",
    "                                           target_size=(224,224), \n",
    "                                           batch_size=32,\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNDgYni5c8Qk"
   },
   "source": [
    "# visualizaing the data that is fed to train data gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5tVIpYKDc2a_",
    "outputId": "c76e7b16-1b3b-4afb-c539-6bf14de8d8c5"
   },
   "outputs": [],
   "source": [
    "# to visualize the images in the traing data denerator \n",
    "\n",
    "t_img , label = train_data.next()\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# function when called will prot the images \n",
    "def plotImages(img_arr, label):\n",
    "  \"\"\"\n",
    "  input  :- images array \n",
    "  output :- plots the images \n",
    "  \"\"\"\n",
    "  count = 0\n",
    "  for im, l in zip(img_arr,label) :\n",
    "    plt.imshow(im)\n",
    "    plt.title(im.shape)\n",
    "    plt.axis = False\n",
    "    plt.show()\n",
    "    \n",
    "    count += 1\n",
    "    if count == 10:\n",
    "      break\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# function call to plot the images \n",
    "plotImages(t_img, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhuAjX8KfPa-"
   },
   "source": [
    "# having early stopping and model check point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_XtYmjt5dZ2c"
   },
   "outputs": [],
   "source": [
    "## having early stopping and model check point \n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# early stopping\n",
    "es = EarlyStopping(monitor='val_accuracy', min_delta= 0.01 , patience= 5, verbose= 1, mode='auto')\n",
    "\n",
    "# model check point\n",
    "mc = ModelCheckpoint(filepath=\"best_model.h5\", monitor= 'val_accuracy', verbose= 1, save_best_only= True, mode = 'auto')\n",
    "\n",
    "# puting call back in a list \n",
    "call_back = [es, mc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4GxQ9-qSfT3K",
    "outputId": "05e315e5-8f3b-481f-b024-5cbfa5f624ef"
   },
   "outputs": [],
   "source": [
    "hist = model.fit_generator(train_data, \n",
    "                           steps_per_epoch= 10, \n",
    "                           epochs= 30, \n",
    "                           validation_data= val_data, \n",
    "                           validation_steps= 8, \n",
    "                           callbacks=[es,mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6YSss56fWrD"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Loading the best fit model \n",
    "from keras.models import load_model\n",
    "model = load_model(\"/content/best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rd0p7xDzfe2V",
    "outputId": "852aa271-31b9-4d8d-8ef8-9de03085381a"
   },
   "outputs": [],
   "source": [
    "h =  hist.history\n",
    "h.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "2Qm3Qz11fhMy",
    "outputId": "3a3cdbdd-167c-4368-c675-8b8290d607a9"
   },
   "outputs": [],
   "source": [
    "plt.plot(h['accuracy'])\n",
    "plt.plot(h['val_accuracy'] , c = \"red\")\n",
    "plt.title(\"acc vs v-acc\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "f2Z9fjdZfjcU",
    "outputId": "1a92fc5a-71a1-4c89-8fef-e92522f3fbcf"
   },
   "outputs": [],
   "source": [
    "plt.plot(h['loss'])\n",
    "plt.plot(h['val_loss'] , c = \"red\")\n",
    "plt.title(\"loss vs v-loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mj02ddxjTyr"
   },
   "outputs": [],
   "source": [
    "# just to map o/p values \n",
    "op = dict(zip( train_data.class_indices.values(), train_data.class_indices.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "8xRgvdZZfltN",
    "outputId": "20dde6d1-1350-4cd7-b671-cd711beeafdc"
   },
   "outputs": [],
   "source": [
    "# path for the image to see if it predics correct class\n",
    "\n",
    "path = \"/content/test/angry/PrivateTest_1054527.jpg\"\n",
    "img = load_img(path, target_size=(224,224) )\n",
    "\n",
    "i = img_to_array(img)/255\n",
    "input_arr = np.array([i])\n",
    "input_arr.shape\n",
    "\n",
    "pred = np.argmax(model.predict(input_arr))\n",
    "\n",
    "print(f\" the image is of {op[pred]}\")\n",
    "\n",
    "# to display the image  \n",
    "plt.imshow(input_arr[0])\n",
    "plt.title(\"input image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woVkimceN8Jr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from keras.preprocessing.image import load_img, img_to_array \n",
    "from keras.models import  load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# load model\n",
    "model = load_model(\"best_model.h5\")\n",
    "\n",
    "\n",
    "face_haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, test_img = cap.read()  # captures frame and returns boolean value and captured image\n",
    "    if not ret:\n",
    "        continue\n",
    "    gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces_detected:\n",
    "        cv2.rectangle(test_img, (x, y), (x + w, y + h), (255, 0, 0), thickness=7)\n",
    "        roi_gray = gray_img[y:y + w, x:x + h]  # cropping region of interest i.e. face area from  image\n",
    "        roi_gray = cv2.resize(roi_gray, (224, 224))\n",
    "        img_pixels = image.img_to_array(roi_gray)\n",
    "        img_pixels = np.expand_dims(img_pixels, axis=0)\n",
    "        img_pixels /= 255\n",
    "\n",
    "        predictions = model.predict(img_pixels)\n",
    "\n",
    "        # find max indexed array\n",
    "        max_index = np.argmax(predictions[0])\n",
    "\n",
    "        emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "        predicted_emotion = emotions[max_index]\n",
    "\n",
    "        cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    resized_img = cv2.resize(test_img, (1000, 700))\n",
    "    cv2.imshow('Facial emotion analysis ', resized_img)\n",
    "\n",
    "    if cv2.waitKey(10) == ord('q'):  # wait until 'q' key is pressed\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Emotion_Detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
